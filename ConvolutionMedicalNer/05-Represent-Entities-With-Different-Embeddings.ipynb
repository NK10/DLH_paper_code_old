{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "# import glove\n",
    "# from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n",
    "w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "fasttext = FastText.load(\"embeddings/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_index_list = []\n",
    "for i in new_notes.itertuples():\n",
    "    \n",
    "    if len(i.ner) == 0:\n",
    "        null_index_list.append(i.Index)\n",
    "new_notes.drop(null_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "med7_ner_data = {}\n",
    "\n",
    "for ii in new_notes.itertuples():\n",
    "    \n",
    "    p_id = ii.SUBJECT_ID\n",
    "    ind = ii.Index\n",
    "    \n",
    "    try:\n",
    "        new_ner = new_notes.loc[ind].ner\n",
    "    except:\n",
    "        new_ner = []\n",
    "            \n",
    "    unique = set()\n",
    "    new_temp = []\n",
    "    \n",
    "    for j in new_ner:\n",
    "        for k in j:\n",
    "            \n",
    "            unique.add(k[0])\n",
    "            new_temp.append(k)\n",
    "\n",
    "    if p_id in med7_ner_data:\n",
    "        for i in new_temp:\n",
    "            med7_ner_data[p_id].append(i)\n",
    "    else:\n",
    "        med7_ner_data[p_id] = new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for ni,te in med7_ner_data.items():\n",
    "    print(ni, te)\n",
    "    print(ttt)\n",
    "\n",
    "\n",
    "dicti = {ni:te}\n",
    "\n",
    "data_types = [dicti]\n",
    "data_names = [\"new_ner\"]\n",
    "\n",
    "for data, names in zip(data_types, data_names):\n",
    "\n",
    "    new_word2vec = {}\n",
    "    print(\"w2vec starting..\")\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(w2vec[i[0]])\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "                print('in w2vec actual',i[0])\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: continue\n",
    "                    avg = np.asarray(avg)\n",
    "                    t = np.asarray(map(mean, zip(*avg)))\n",
    "                    patient_temp.append(t)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_word2vec[k] = patient_temp\n",
    "\n",
    "    #############################################################################\n",
    "    print(\"fasttext starting..\")\n",
    "        \n",
    "    new_fasttextvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(fasttext[i[0]])\n",
    "            except:\n",
    "                print('in fasttext',i)\n",
    "                pass\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    #############################################################################    \n",
    "        \n",
    "    print(\"combined starting..\")\n",
    "    new_concatvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "        patient_temp = []\n",
    "    #     if k != 6: continue\n",
    "        for i in v:\n",
    "            w2vec_temp = []\n",
    "            fasttemp = []\n",
    "            try:\n",
    "                w2vec_temp = w2vec[i[0]]\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: \n",
    "                        w2vec_temp = [0] * 100\n",
    "                    else:\n",
    "                        avg = np.asarray(avg)\n",
    "                        print('combined function avg value',avg)\n",
    "                        w2vec_temp = np.asarray(list(map(mean, zip(*avg))))\n",
    "                else:\n",
    "                    w2vec_temp = [0] * 100\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                fasttemp = fasttext[i[0]]\n",
    "            except:\n",
    "                pass\n",
    "            if len(fasttemp) == 0: \n",
    "                pass\n",
    "            print('fasttemp{0}'.format(len(fasttemp)))\n",
    "            print('w2vec_temp{0}'.format((w2vec_temp)))\n",
    "            \n",
    "            appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "            patient_temp.append(appended)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_concatvec[k] = patient_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vec starting..\n",
      "fasttext starting..\n",
      "combined starting..\n",
      "21731 21809 21837\n"
     ]
    }
   ],
   "source": [
    "data_types = [med7_ner_data]\n",
    "data_names = [\"new_ner\"]\n",
    "\n",
    "for data, names in zip(data_types, data_names):\n",
    "\n",
    "    new_word2vec = {}\n",
    "    print(\"w2vec starting..\")\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(w2vec[i[0]])\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: continue\n",
    "                    avg = np.asarray(avg)\n",
    "                    t = np.asarray(list(map(mean, zip(*avg)))) # nitin added list after map for fixing the return of map as hash\n",
    "                    patient_temp.append(t)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_word2vec[k] = patient_temp\n",
    "\n",
    "    #############################################################################\n",
    "    print(\"fasttext starting..\")\n",
    "        \n",
    "    new_fasttextvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(fasttext[i[0]])\n",
    "            except:\n",
    "                pass\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    #############################################################################    \n",
    "        \n",
    "    print(\"combined starting..\")\n",
    "    new_concatvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "        patient_temp = []\n",
    "    #     if k != 6: continue\n",
    "        for i in v:\n",
    "            w2vec_temp = []\n",
    "            fasttemp = [] # nitin added \n",
    "            try:\n",
    "                w2vec_temp = w2vec[i[0]]\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: \n",
    "                        w2vec_temp = [0] * 100\n",
    "                    else:\n",
    "                        avg = np.asarray(avg)\n",
    "                        w2vec_temp = np.asarray(list(map(mean, zip(*avg)))) # nitin added list after map for fixing the return of map as hash \n",
    "                else:\n",
    "                    w2vec_temp = [0] * 100\n",
    "            #print('value for i',i)    # nitin added the try pass to eliminate the error if i[[0]] doesn't exist\n",
    "            try:\n",
    "                fasttemp = fasttext[i[0]]\n",
    "            except:\n",
    "                fasttemp = [0] * 100\n",
    "            #print('fasttemp {0}, w2vec_temp{1},'.format(fasttemp,w2vec_temp))\n",
    "            appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "            patient_temp.append(appended)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_concatvec[k] = patient_temp\n",
    "\n",
    "    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n",
    "    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")\n",
    "    pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")\n",
    "    pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new_ner'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, dict, dict)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_word2vec),type(new_fasttextvec),type(new_concatvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nitin added with a guess work as the *_dict variable not exist \n",
    "new_fasttext_dict = new_fasttextvec.copy()\n",
    "new_word2vec_dict =  new_word2vec.copy()\n",
    "new_combined_dict = new_concatvec.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21731 21731 21759\n"
     ]
    }
   ],
   "source": [
    "diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n",
    "\n",
    "for i in diff:\n",
    "    del new_fasttext_dict[i]\n",
    "    del new_combined_dict[i]\n",
    "print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n",
    "\n",
    "\n",
    "pd.to_pickle(new_word2vec_dict, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n",
    "pd.to_pickle(new_fasttext_dict, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n",
    "pd.to_pickle(new_combined_dict, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
